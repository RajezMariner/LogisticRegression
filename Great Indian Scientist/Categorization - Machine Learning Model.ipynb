{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the product category"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) EDA - Exploratory Data Analysis\n",
    "2) Data Pre-Processing\n",
    "3) Using NLP librarires for getting the product transmission\n",
    "4) Decision of ML models\n",
    "5) Accuracy and Performance metrics to understand the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = pd.read_csv(\"E:/Projects/MachineLearning/Machine-Learning/Hacker Earth/Great Indian Scientist/Dataset/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Inv_Id', 'Vendor_Code', 'GL_Code', 'Inv_Amt', 'Item_Description',\n",
       "       'Product_Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5566, 6)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83.24\n",
       "1    51.18\n",
       "Name: Inv_Amt, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data['Inv_Amt'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15001\n",
       "1    15002\n",
       "Name: Inv_Id, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.Inv_Id[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83.24\n",
       "1    51.18\n",
       "Name: Inv_Amt, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.Inv_Amt[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLASS-1963', 'CLASS-1250', 'CLASS-1274', ..., 'CLASS-1721',\n",
       "       'CLASS-1652', 'CLASS-1758'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.Product_Category.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us convert the string data to numberical data. Since it is going to be the categorical data, we will have to specity the number here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "\n",
    "product_desc = product_data.Item_Description\n",
    "target = product_data.Product_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and preprocessing is started\n",
      "\n",
      "Data cleaning and preprocessing is completed\n"
     ]
    }
   ],
   "source": [
    "month = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "data = []\n",
    "print('Data cleaning and preprocessing is started')\n",
    "for index,item in product_desc.items():\n",
    "    item = item.replace('\\\\',' ')\n",
    "    item = item.replace('/',' ')\n",
    "    item = item.replace('''(Field Only)''','')\n",
    "    item = re.sub(r\"[^a-zA-Z0-9]+\", ' ',item)\n",
    "    item = re.sub(\"\\d+\",\"\",item)\n",
    "    item = item.replace('.',' ')\n",
    "    item = item.lower()\n",
    "    data.append(item)\n",
    "\n",
    "print('')\n",
    "print('Data cleaning and preprocessing is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us go ahead with the stop words of English. Since the product description does not need to deal with the stop words, I would\n",
    "# like to go ahead with the stop words implementation.. We will add the months in the stop words in the list.\n",
    "# Month looks not useful in the prediction of the product category.\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + month\n",
    "\n",
    "\n",
    "#We will go ahead with the word tokenizer. Not the sentence tokenizer.\n",
    "final_data = []\n",
    "for item in data:\n",
    "    \n",
    "    tokenize = word_tokenize(text=item)\n",
    "    words = [ w for w in tokenize if not w in stop_words]\n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        text = text + w + \" \"\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    final_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check the balanced vs imbalanced data... bu using the different selection methods.\n",
    "product_data['Pre_Processed_Data'] = final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let`s break up the target variable as well...\n",
    "target = []\n",
    "for pc in product_data['Product_Category'].values:\n",
    "    target.append(pc.replace(\"CLASS-\",\"\"))\n",
    "product_data['Target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inv_Id</th>\n",
       "      <th>Vendor_Code</th>\n",
       "      <th>GL_Code</th>\n",
       "      <th>Inv_Amt</th>\n",
       "      <th>Item_Description</th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>Pre_Processed_Data</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15001</td>\n",
       "      <td>VENDOR-1676</td>\n",
       "      <td>GL-6100410</td>\n",
       "      <td>83.24</td>\n",
       "      <td>Artworking/Typesetting Production Jun 2009 Champion Parts Inc SMAP Prototype and Comp Production/Packaging Design</td>\n",
       "      <td>CLASS-1963</td>\n",
       "      <td>artworking typesetting production champion parts inc smap prototype comp production packaging design</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15002</td>\n",
       "      <td>VENDOR-1883</td>\n",
       "      <td>GL-2182000</td>\n",
       "      <td>51.18</td>\n",
       "      <td>Auto Leasing Corporate Services Corning Inc /Ny 2013-Mar  Auto Leasing and Maintenance Other Corporate Services</td>\n",
       "      <td>CLASS-1250</td>\n",
       "      <td>auto leasing corporate services corning inc ny auto leasing maintenance corporate services</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Inv_Id  Vendor_Code     GL_Code  Inv_Amt  \\\n",
       "0   15001  VENDOR-1676  GL-6100410    83.24   \n",
       "1   15002  VENDOR-1883  GL-2182000    51.18   \n",
       "\n",
       "                                                                                                    Item_Description  \\\n",
       "0  Artworking/Typesetting Production Jun 2009 Champion Parts Inc SMAP Prototype and Comp Production/Packaging Design   \n",
       "1    Auto Leasing Corporate Services Corning Inc /Ny 2013-Mar  Auto Leasing and Maintenance Other Corporate Services   \n",
       "\n",
       "  Product_Category  \\\n",
       "0       CLASS-1963   \n",
       "1       CLASS-1250   \n",
       "\n",
       "                                                                                     Pre_Processed_Data  \\\n",
       "0  artworking typesetting production champion parts inc smap prototype comp production packaging design   \n",
       "1            auto leasing corporate services corning inc ny auto leasing maintenance corporate services   \n",
       "\n",
       "  Target  \n",
       "0   1963  \n",
       "1   1250  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final data contains the words without stop words and removed months.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of words..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_te, X_sc, Y_sc = train_test_split(final_data,target,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWordsWithTrainTestData(train_data, test_data):\n",
    "    count_vect = CountVectorizer(stop_words='english',max_features=500,min_df=10) #in scikit-learn\n",
    "    \n",
    "    train_data_vectorized = count_vect.fit_transform(train_data)\n",
    "    print(\"some feature names after transforming the TRAIN data\", count_vect.get_feature_names()[:10])\n",
    "    \n",
    "    test_data_vectorized = count_vect.transform(test_data)\n",
    "    print(\"some feature names after transforming the TEST data\", count_vect.get_feature_names()[:10])\n",
    "    \n",
    "    print(\"the type of count vectorizer \",type(train_data_vectorized))\n",
    "    print(\"the shape of out text BOW vectorizer \",train_data_vectorized.get_shape())\n",
    "    \n",
    "    print(\"the type of count vectorizer \",type(test_data_vectorized))\n",
    "    print(\"the shape of out text BOW vectorizer \",test_data_vectorized.get_shape())\n",
    "    \n",
    "    return count_vect, train_data_vectorized, test_data_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names after transforming the TRAIN data ['account', 'adr', 'adv', 'advertising', 'agency', 'air', 'akorn', 'akzo', 'alabama', 'alco']\n",
      "some feature names after transforming the TEST data ['account', 'adr', 'adv', 'advertising', 'agency', 'air', 'akorn', 'akzo', 'alabama', 'alco']\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (3896, 250)\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (1670, 250)\n"
     ]
    }
   ],
   "source": [
    "count_vect,train_data_vect, test_data_vect =  bagOfWordsWithTrainTestData(train_data=X_tr,test_data=Y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSimpleCV_On_Log_Regression(penalty, train_data, test_data, train_score, test_score):\n",
    "    \n",
    "    regularization_coeff = [10**-4, 10**-3, 10**-2, 10**-1, 10**0, 10**1, 10**2, 10**3, 10**4]\n",
    "    \n",
    "    score_m1 = []\n",
    "    score_m2 = []\n",
    "    \n",
    "    for co_eff in regularization_coeff:\n",
    "        \n",
    "        logistic_model = LogisticRegression(penalty=penalty,C=co_eff,multi_class='ovr',class_weight='balanced')\n",
    "        logistic_model.fit(X=train_data,y=train_score)\n",
    "        \n",
    "        predicted_data_m1 = logistic_model.predict(X=test_data)\n",
    "        \n",
    "        model_accuracy = 0\n",
    "        count=0;\n",
    "        for i in range(len(predicted_data_m1)):\n",
    "            if(predicted_data_m1[i]==test_score[i]):\n",
    "                count+=1\n",
    "        print('The accuracy of the model is %.2f for the co_eff  %.4f and the number of correct values is %d'%(count/len(test_score),co_eff,count))\n",
    "        \n",
    "        score_m1.append(predicted_data_m1)\n",
    "        \n",
    "    return score_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performOptimalLogisticRegression(test_data,train_data,train_score,coeff,penalty):\n",
    "    \n",
    "    logistic_model = LogisticRegression(penalty=penalty,C=coeff,multi_class='ovr',class_weight='balanced')\n",
    "    logistic_model.fit(X=train_data,y=train_score)\n",
    "    predicted_output = logistic_model.predict(X=test_data)\n",
    "    return predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.00 for the co_eff  0.0001 and the number of correct values is 1\n",
      "The accuracy of the model is 0.27 for the co_eff  0.0010 and the number of correct values is 449\n",
      "The accuracy of the model is 0.98 for the co_eff  0.0100 and the number of correct values is 1638\n",
      "The accuracy of the model is 1.00 for the co_eff  0.1000 and the number of correct values is 1664\n",
      "The accuracy of the model is 1.00 for the co_eff  1.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  10.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  100.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  1000.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  10000.0000 and the number of correct values is 1665\n"
     ]
    }
   ],
   "source": [
    "score_m1 = performSimpleCV_On_Log_Regression(penalty='l1',\n",
    "    train_data=train_data_vect, test_data=test_data_vect,\n",
    "    train_score=X_sc, test_score=Y_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.94 for the co_eff  0.0001 and the number of correct values is 1563\n",
      "The accuracy of the model is 0.97 for the co_eff  0.0010 and the number of correct values is 1628\n",
      "The accuracy of the model is 0.99 for the co_eff  0.0100 and the number of correct values is 1660\n",
      "The accuracy of the model is 1.00 for the co_eff  0.1000 and the number of correct values is 1663\n",
      "The accuracy of the model is 1.00 for the co_eff  1.0000 and the number of correct values is 1664\n",
      "The accuracy of the model is 1.00 for the co_eff  10.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  100.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  1000.0000 and the number of correct values is 1665\n",
      "The accuracy of the model is 1.00 for the co_eff  10000.0000 and the number of correct values is 1665\n"
     ]
    }
   ],
   "source": [
    "score_m1 = performSimpleCV_On_Log_Regression(penalty='l2',\n",
    "    train_data=train_data_vect, test_data=test_data_vect,\n",
    "    train_score=X_sc, test_score=Y_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation...!\n",
    "\n",
    "From the above model, we could conclude that the model - Logistics Regression could be used very well for the given data set.\n",
    "\n",
    "The model works fine with both the regularizer. Hence, I would like to choose \"L1\" regularizer with 10 co-efficients.\n",
    "\n",
    "Now, use the same model configuration for predicting the test data....\n",
    "\n",
    "Test data will undergo the same pre-processing technique....!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"E:/Projects/MachineLearning/Machine-Learning/Hacker Earth/Great Indian Scientist/Dataset/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and preprocessing is started\n",
      "\n",
      "Data cleaning and preprocessing is completed\n"
     ]
    }
   ],
   "source": [
    "month = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "test_data = []\n",
    "print('Data cleaning and preprocessing is started')\n",
    "for index,item in test_df.Item_Description.items():\n",
    "    item = item.replace('\\\\',' ')\n",
    "    item = item.replace('/',' ')\n",
    "    item = item.replace('''(Field Only)''','')\n",
    "    item = re.sub(r\"[^a-zA-Z0-9]+\", ' ',item)\n",
    "    item = re.sub(\"\\d+\",\"\",item)\n",
    "    item = item.replace('.',' ')\n",
    "    item = item.lower()\n",
    "    test_data.append(item)\n",
    "\n",
    "print('')\n",
    "print('Data cleaning and preprocessing is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us go ahead with the stop words of English. Since the product description does not need to deal with the stop words, I would\n",
    "# like to go ahead with the stop words implementation.. We will add the months in the stop words in the list.\n",
    "# Month looks not useful in the prediction of the product category.\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + month\n",
    "\n",
    "\n",
    "#We will go ahead with the word tokenizer. Not the sentence tokenizer.\n",
    "final_test_data = []\n",
    "for item in test_data:\n",
    "    tokenize = word_tokenize(text=item)\n",
    "    words = [ w for w in tokenize if not w in stop_words]\n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        text = text + w + \" \"\n",
    "    text = text.strip()\n",
    "    final_test_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Final_Pre_Processed_Data'] = final_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names after transforming the TRAIN data ['account', 'acme', 'adr', 'adv', 'advertising', 'agency', 'air', 'airtex', 'akorn', 'akzo']\n",
      "some feature names after transforming the TEST data ['account', 'acme', 'adr', 'adv', 'advertising', 'agency', 'air', 'airtex', 'akorn', 'akzo']\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (5566, 301)\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (2446, 301)\n"
     ]
    }
   ],
   "source": [
    "count_vect,train_data_vect, test_data_vect =  bagOfWordsWithTrainTestData(train_data=final_data,test_data=final_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = performOptimalLogisticRegression(test_data=test_data_vect,train_data=train_data_vect,train_score=target,penalty='l1',coeff=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted = [\"CLASS-\"+i for i in predicted_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Product_Category'] = class_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inv_Id</th>\n",
       "      <th>Vendor_Code</th>\n",
       "      <th>GL_Code</th>\n",
       "      <th>Inv_Amt</th>\n",
       "      <th>Item_Description</th>\n",
       "      <th>Final_Pre_Processed_Data</th>\n",
       "      <th>Product_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15003</td>\n",
       "      <td>VENDOR-2513</td>\n",
       "      <td>GL-6050310</td>\n",
       "      <td>56.13</td>\n",
       "      <td>Travel and Entertainment Miscellaneous Company Car (Field Only) Ground Transportation Miscellaneous Company Car (Field Only) Oct2011 Fortune National Corp</td>\n",
       "      <td>travel entertainment miscellaneous company car ground transportation miscellaneous company car fortune national corp</td>\n",
       "      <td>CLASS-1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15008</td>\n",
       "      <td>VENDOR-1044</td>\n",
       "      <td>GL-6101400</td>\n",
       "      <td>96.56</td>\n",
       "      <td>Final Site Clean Up Store Construction Advanced Micro Devices Inc Oct2011 General Requirements General Contractor</td>\n",
       "      <td>final site clean store construction advanced micro devices inc general requirements general contractor</td>\n",
       "      <td>CLASS-1522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Inv_Id  Vendor_Code     GL_Code  Inv_Amt  \\\n",
       "0   15003  VENDOR-2513  GL-6050310    56.13   \n",
       "1   15008  VENDOR-1044  GL-6101400    96.56   \n",
       "\n",
       "                                                                                                                                             Item_Description  \\\n",
       "0  Travel and Entertainment Miscellaneous Company Car (Field Only) Ground Transportation Miscellaneous Company Car (Field Only) Oct2011 Fortune National Corp   \n",
       "1                                           Final Site Clean Up Store Construction Advanced Micro Devices Inc Oct2011 General Requirements General Contractor   \n",
       "\n",
       "                                                                                               Final_Pre_Processed_Data  \\\n",
       "0  travel entertainment miscellaneous company car ground transportation miscellaneous company car fortune national corp   \n",
       "1                final site clean store construction advanced micro devices inc general requirements general contractor   \n",
       "\n",
       "  Product_Category  \n",
       "0       CLASS-1758  \n",
       "1       CLASS-1522  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.DataFrame({'Inv_Id':test_df['Inv_Id'],'Product_Category':test_df['Product_Category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.to_csv('E:/Projects/MachineLearning/Machine-Learning/Hacker Earth/Great Indian Scientist/Final_Submisson.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
